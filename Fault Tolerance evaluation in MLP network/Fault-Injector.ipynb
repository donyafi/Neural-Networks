{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from struct import pack, unpack\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras import backend as K\n",
    "import random, math\n",
    "from enum import  Enum\n",
    "import sys\n",
    "from MLP import MLP\n",
    "\n",
    "RandomBitPos=-1\n",
    "class InjectionType(Enum):\n",
    "\tWeights=0\n",
    "\tOutputs=1\n",
    "class BitFaultType(Enum):\n",
    "\tZero=0\n",
    "\tRand=1\n",
    "\tFlip=2\n",
    "class LayerSelectionType(Enum):\n",
    "\tSingle_Layer=0\n",
    "\tMultiple_Layer=1\n",
    "\n",
    "def bitflip(f, pos):\n",
    "\t\n",
    "\t\"\"\" Single bit-flip in 32 bit floats \"\"\"\n",
    "\n",
    "\tf_ = pack('f', f)\n",
    "\tb = list(unpack('BBBB', f_))\n",
    "\t[q, r] = divmod(pos, 8)\n",
    "\tb[q] ^= 1 << r\n",
    "\tf_ = pack('BBBB', *b)\n",
    "\tf = unpack('f', f_)\n",
    "\treturn f[0]\n",
    "\n",
    "\n",
    "def inject(mlp:MLP,Injection:InjectionType,FType:LayerSelectionType,BitFault:BitFaultType,FaultCount:int, log_level=\"ERROR\"):\n",
    "\t# Logging setup\n",
    "\tlogging.basicConfig()\n",
    "\tlogging.getLogger().setLevel(log_level)\n",
    "\tlogging.debug(\"Logging level set to {0}\".format(log_level))\n",
    "\t#self.Model = mlp.Model # No more passing or using a session variable in TF v2\n",
    "\t# Call the corresponding FI function\n",
    "\tif(Injection==InjectionType.Weights):\n",
    "\t\tlayer_states(mlp.Model,FType,BitFault,FaultCount,RandomBitPos,mlp.x_test_images)\n",
    "\telif(Injection==InjectionType.Outputs):\n",
    "\t\tsdc=0.0\n",
    "\t\tres=None\n",
    "\t\tfor i in range(1,len(mlp.x_test_images)):\n",
    "\t\t\tres=layer_outputs(mlp.Model,FType,BitFault,FaultCount,RandomBitPos,mlp.x_test_images[i:i+1])\n",
    "\t\t\tif(i%200==0):\n",
    "\t\t\t\tShowPercentage((float(i)/len(mlp.y_test_labels))*100)\n",
    "\t\t\tif(res != mlp.y_test_labels[i:i+1]):\n",
    "\t\t\t\tsdc = sdc + 1.\n",
    "\t\tShowPercentage(100)\n",
    "\t\ttest_acc=1.0-sdc/len(mlp.y_test_labels)\n",
    "\t\t#print(\"Accuracy after faults:\", test_acc)\n",
    "\t\treturn test_acc\n",
    "\n",
    "def layer_states(model, FType:LayerSelectionType,BitFault:BitFaultType,FaultCount:int,FaultyBitPos:int=RandomBitPos, Dataset=None):\n",
    "\t\t\n",
    "\t\"\"\" FI in layer states \"\"\"\n",
    "\t\t\n",
    "\tif(FType == LayerSelectionType.Single_Layer):\n",
    "\n",
    "\t\t\"\"\" Single layer fault injection mode \"\"\"\n",
    "\n",
    "\t\tlogging.info(\"Starting fault injection in a random layer\")\n",
    "\n",
    "\t\t# Retrieve type and amount of fault\n",
    "\t\t\t\n",
    "\t\tfiSz = FaultCount\n",
    "\n",
    "\t\t# Choose a random layer for injection\n",
    "\t\trandnum = random.randint(0, len(model.trainable_variables) - 1)\n",
    "\n",
    "\t\t# Get layer states info\n",
    "\t\tv = model.trainable_variables[randnum]\n",
    "\t\tnum = v.shape.num_elements()\n",
    "\n",
    "\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\tfiSz = (fiSz * num) / 100\n",
    "\t\t\tfiSz = math.floor(fiSz)\n",
    "\n",
    "\t\t# Choose the indices for FI\n",
    "\t\tind = random.sample(range(num), fiSz)\n",
    "\n",
    "\t\t# Unstack elements into a single dimension\n",
    "\t\telem_shape = v.shape\n",
    "\t\tv_ = tf.identity(v)\n",
    "\t\tv_ = tf.keras.backend.flatten(v_)\n",
    "\t\tv_ = tf.unstack(v_)\n",
    "\n",
    "\t\t# Inject the specified fault into the randomly chosen values\n",
    "\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\tfor item in ind:\n",
    "\t\t\t\tv_[item] = 0.\n",
    "\t\telif(BitFault == BitFaultType.Rand):\n",
    "\t\t\tfor item in ind:\n",
    "\t\t\t\tv_[item] = np.random.random()\n",
    "\t\telif(BitFault == BitFaultType):\n",
    "\t\t\tfor item in ind:\n",
    "\t\t\t\tval = v_[item]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# If random bit chosen to be flipped\n",
    "\t\t\t\tif(FaultyBitPos == RandomBitPos):\n",
    "\t\t\t\t\tpos = random.randint(0, 31)\n",
    "\n",
    "\t\t\t\t# If bit position specified for flip\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpos = int(FaultyBitPos)\n",
    "\t\t\t\tval_ = bitflip(val, pos)\n",
    "\t\t\t\tv_[item] = val_\n",
    "\n",
    "\t\t# Reshape into original dimensions and store the faulty tensor\n",
    "\t\tv_ = tf.stack(v_)\n",
    "\t\tv_ = tf.reshape(v_, elem_shape)\n",
    "\t\tv.assign(v_)\n",
    "\n",
    "\t\tlogging.info(\"Completed injections... exiting\")\n",
    "\n",
    "\telif(FType == LayerSelectionType.Multiple_Layer):\n",
    "\n",
    "\t\t\"\"\" Multiple layer fault injection mode \"\"\"\n",
    "\n",
    "\t\tlogging.info(\"Starting fault injection in all layers\")\n",
    "\n",
    "\t\t# Retrieve type and amount of fault\n",
    "\t\t\t\n",
    "\t\tfiSz = FaultCount\n",
    "\n",
    "\t\t# Loop through each available layer in the model\n",
    "\t\tfor n in range(len(model.trainable_variables) - 1):\n",
    "\n",
    "\t\t\t# Get layer states info\n",
    "\t\t\tv = model.trainable_variables[n]\n",
    "\t\t\tnum = v.shape.num_elements()\n",
    "\n",
    "\t\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\t\tfiSz = (fiSz * num) / 100\n",
    "\t\t\t\tfiSz = math.floor(fiSz)\n",
    "\n",
    "\t\t\t# Choose the indices for FI\n",
    "\t\t\tind = random.sample(range(num), fiSz)\n",
    "\n",
    "\t\t\t# Unstack elements into a single dimension\n",
    "\t\t\telem_shape = v.shape\n",
    "\t\t\tv_ = tf.identity(v)\n",
    "\t\t\tv_ = tf.keras.backend.flatten(v_)\n",
    "\t\t\tv_ = tf.unstack(v_)\n",
    "\n",
    "\t\t\t# Inject the specified fault into the randomly chosen values\n",
    "\t\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\t\tfor item in ind:\n",
    "\t\t\t\t\tv_[item] = 0.\n",
    "\t\t\telif(BitFault == BitFaultType.Rand):\n",
    "\t\t\t\tfor item in ind:\n",
    "\t\t\t\t\tv_[item] = np.random.random()\n",
    "\t\t\telif(BitFault == BitFaultType.Flip):\n",
    "\t\t\t\tfor item in ind:\n",
    "\t\t\t\t\tval = v_[item]\n",
    "\n",
    "\t\t\t\t\t# If random bit chosen to be flipped\n",
    "\t\t\t\t\tif(FaultyBitPos == RandomBitPos):\n",
    "\t\t\t\t\t\tpos = random.randint(0, 31)\n",
    "\n",
    "\t\t\t\t\t# If bit position specified for flip\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tpos = int(FaultyBitPos)\n",
    "\t\t\t\t\tval_ = bitflip(val, pos)\n",
    "\t\t\t\t\tv_[item] = val_\n",
    "\n",
    "\t\t\t# Reshape into original dimensions and store the faulty tensor\n",
    "\t\t\tv_ = tf.stack(v_)\n",
    "\t\t\tv_ = tf.reshape(v_, elem_shape)\n",
    "\t\t\tv.assign(v_)\n",
    "\n",
    "\t\tlogging.info(\"Completed injections... exiting\")\n",
    "\n",
    "\n",
    "def layer_outputs(model,FType:LayerSelectionType,BitFault:BitFaultType,FaultCount:int,FaultyBitPos:int=RandomBitPos, Dataset_Sample=None):\n",
    "\n",
    "\t\"\"\" FI in layer computations/outputs \"\"\"\n",
    "\n",
    "\tif(FType == LayerSelectionType.Single_Layer):\n",
    "\n",
    "\t\t\"\"\" Single layer fault injection mode \"\"\"\n",
    "\n",
    "\t\tlogging.info(\"Starting fault injection in a random layer\")\n",
    "\n",
    "\t\t# Retrieve type and amount of fault\n",
    "\t\t\t\n",
    "\t\tfiSz = FaultCount\n",
    "\n",
    "\t\t# Get the input for which dynamic injection is to be done\n",
    "\t\tx_test = Dataset_Sample\n",
    "\n",
    "\t\t# Choose a random layer for injection\n",
    "\t\trandnum = random.randint(0, len(model.layers) - 2)\n",
    "\n",
    "\t\tfiLayer = model.layers[randnum]\n",
    "\n",
    "\t\t# Get the outputs of the chosen layer\n",
    "\t\tget_output = K.function([model.layers[0].input], [fiLayer.output])\n",
    "\t\tfiLayerOutputs = get_output([x_test])\n",
    "\n",
    "\t\t# Unstack elements into a single dimension\n",
    "\t\telem_shape = fiLayerOutputs[0].shape\n",
    "\t\tfiLayerOutputs[0] = fiLayerOutputs[0].flatten()\n",
    "\t\tnum = fiLayerOutputs[0].shape[0]\n",
    "\n",
    "\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\tfiSz = (fiSz * num) / 100\n",
    "\t\t\tfiSz = math.floor(fiSz)\n",
    "\n",
    "\t\t# Choose the indices for FI\n",
    "\t\tind = random.sample(range(num), fiSz)\n",
    "\n",
    "\t\t# Inject the specified fault into the randomly chosen values\n",
    "\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\tfor item in ind:\n",
    "\t\t\t\tfiLayerOutputs[0][item] = 0.\n",
    "\t\telif(BitFault == BitFaultType.Rand):\n",
    "\t\t\tfor item in ind:\n",
    "\t\t\t\tfiLayerOutputs[0][item] = np.random.random()\n",
    "\t\telif(BitFault == BitFaultType.Flip):\n",
    "\t\t\tfor item in ind:\n",
    "\t\t\t\tval = fiLayerOutputs[0][item]\n",
    "\t\t\t\tif(FaultyBitPos == RandomBitPos):\n",
    "\t\t\t\t\tpos = random.randint(0, 31)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpos = int(FaultyBitPos)\n",
    "\t\t\t\tval_ = bitflip(val, pos)\n",
    "\t\t\t\tfiLayerOutputs[0][item] = val_\n",
    "\n",
    "\t\t# Reshape into original dimensions and get the final prediction\n",
    "\t\tfiLayerOutputs[0] = fiLayerOutputs[0].reshape(elem_shape)\n",
    "\t\tget_pred = K.function([model.layers[randnum + 1].input], [model.layers[-1].output])\n",
    "\t\tpred = get_pred([fiLayerOutputs])\n",
    "\n",
    "\t\t# Uncomment below line and comment next two lines for ImageNet models\n",
    "\t\t# return pred\n",
    "\t\tlabels = np.argmax(pred, axis=-1)\n",
    "\t\treturn labels[0]\n",
    "\t\t\t\n",
    "\t\tlogging.info(\"Completed injections... exiting\")\n",
    "\n",
    "\telif(FType == LayerSelectionType.Multiple_Layer):\n",
    "\n",
    "\t\t\"\"\" Multiple layer fault injection mode \"\"\"\n",
    "\n",
    "\t\tlogging.info(\"Starting fault injection in all layers\")\n",
    "\n",
    "\t\t# Retrieve type and amount of fault\n",
    "\t\t\t\n",
    "\t\tfiSz = FaultCount\n",
    "\n",
    "\t\t# Get the input for which dynamic injection is to be done\n",
    "\t\tx_test = Dataset_Sample\n",
    "\n",
    "\t\t# Get the outputs of the first layer\n",
    "\t\tget_output_0 = K.function([model.layers[0].input], [model.layers[1].output])\n",
    "\t\tfiLayerOutputs = get_output_0([x_test])\n",
    "\n",
    "\t\t# Loop through each available layer in the model\n",
    "\t\tfor n in range(1, len(model.layers) - 2):\n",
    "\n",
    "\t\t\t# Unstack elements into a single dimension\n",
    "\t\t\telem_shape = fiLayerOutputs[0].shape\n",
    "\t\t\tfiLayerOutputs[0] = fiLayerOutputs[0].flatten()\n",
    "\t\t\tnum = fiLayerOutputs[0].shape[0]\n",
    "\t\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\t\tfiSz = (fiSz * num) / 100\n",
    "\t\t\t\tfiSz = math.floor(fiSz)\n",
    "\n",
    "\t\t\t# Choose the indices for FI\n",
    "\t\t\tind = random.sample(range(num), fiSz)\n",
    "\n",
    "\t\t\t# Inject the specified fault into the randomly chosen values\n",
    "\t\t\tif(BitFault == BitFaultType.Zero):\n",
    "\t\t\t\tfor item in ind:\n",
    "\t\t\t\t\tfiLayerOutputs[0][item] = 0.\n",
    "\t\t\telif(BitFault == BitFaultType.Rand):\n",
    "\t\t\t\tfor item in ind:\n",
    "\t\t\t\t\tfiLayerOutputs[0][item] = np.random.random()\n",
    "\t\t\telif(BitFault == BitFaultType.Flip):\n",
    "\t\t\t\tfor item in ind:\n",
    "\t\t\t\t\tval = fiLayerOutputs[0][item]\n",
    "\t\t\t\t\tif(FaultyBitPos == RandomBitPos):\n",
    "\t\t\t\t\t\tpos = random.randint(0, 31)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tpos = int(FaultyBitPos)\n",
    "\t\t\t\t\tval_ = bitflip(val, pos)\n",
    "\t\t\t\t\tfiLayerOutputs[0][item] = val_\n",
    "\n",
    "\t\t\t# Reshape into original dimensions\n",
    "\t\t\tfiLayerOutputs[0] = fiLayerOutputs[0].reshape(elem_shape)\n",
    "\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tCheck if last but one layer reached;\n",
    "\t\t\tif not, replace fiLayerOutputs with the next prediction to continue\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tif(n != (len(model.layers) - 3)):\n",
    "\t\t\t\tget_output = K.function([model.layers[n+1].input], [model.layers[n+2].output])\n",
    "\t\t\t\tfiLayerOutputs = get_output([fiLayerOutputs])\n",
    "\n",
    "\t\t\t# Get final prediction\n",
    "\t\t\tget_pred = K.function([model.layers[len(model.layers)-1].input], [model.layers[-1].output])\n",
    "\t\t\tpred = get_pred([fiLayerOutputs])\n",
    "\n",
    "\t\t\t# Uncomment below line and comment next two lines for ImageNet models\n",
    "\t\t\t# return pred\n",
    "\t\t\tlabels = np.argmax(pred, axis=-1)\n",
    "\t\t\treturn labels[0]\n",
    "\t\t\t\t\n",
    "\t\t\tlogging.info(\"Completed injections... exiting\")\t\t\t\t\n",
    "\n",
    "def ShowPercentage(percent:int):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Fault Injection : %'+str(percent))\n",
    "    sys.stdout.flush()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network...\n",
      "dataset is ready\n",
      "results for  10 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 30ms/step - loss: 0.0830 - accuracy: 0.9769\n",
      "results for  11 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0839 - accuracy: 0.9766\n",
      "results for  12 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0866 - accuracy: 0.9757\n",
      "results for  13 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.0918 - accuracy: 0.9747\n",
      "results for  14 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.0931 - accuracy: 0.9746\n",
      "results for  15 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0912 - accuracy: 0.9747\n"
     ]
    }
   ],
   "source": [
    "def Weight_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork()\n",
    "    model.LoadModel(r'model\\model.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Weights, FType=LayerSelectionType.Multiple_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        model.Test()\n",
    " \n",
    "        \n",
    "Weight_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weight_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork()\n",
    "    model.LoadModel(r'model\\model.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Weights, FType=LayerSelectionType.Single_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        model.Test()\n",
    " \n",
    "        \n",
    "Weight_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network...\n",
      "dataset is ready\n",
      "Fault Injection : %100099999999999994results for  10 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  11 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  12 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  13 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  14 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  15 fault in weights and single layer\n"
     ]
    }
   ],
   "source": [
    "def Output_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork()\n",
    "    model.LoadModel(r'model\\model.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Outputs, FType=LayerSelectionType.Single_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        model.Test()\n",
    "\n",
    " \n",
    "        \n",
    "Output_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Output_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork()\n",
    "    model.LoadModel(r'model\\model.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Weights, FType=LayerSelectionType.Multiple_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        model.Test()\n",
    " \n",
    "        \n",
    "Output_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network...\n",
      "dataset is ready\n",
      "Fault Injection : %100099999999999994results for  10 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 30ms/step - loss: 0.0835 - accuracy: 0.9768\n",
      "Fault Injection : %100099999999999994results for  11 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.0835 - accuracy: 0.9768\n",
      "Fault Injection : %100099999999999994results for  12 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0835 - accuracy: 0.9768\n",
      "Fault Injection : %100099999999999994results for  13 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 13s 40ms/step - loss: 0.0835 - accuracy: 0.9768\n",
      "Fault Injection : %100099999999999994results for  14 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 11s 35ms/step - loss: 0.0835 - accuracy: 0.9768\n",
      "Fault Injection : %100099999999999994results for  15 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 11s 35ms/step - loss: 0.0835 - accuracy: 0.9768\n"
     ]
    }
   ],
   "source": [
    "def Output_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork()\n",
    "    model.LoadModel(r'model\\model.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Outputs, FType=LayerSelectionType.Multiple_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        model.Test()\n",
    " \n",
    "        \n",
    "Output_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Output_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork()\n",
    "    model.LoadModel(r'model\\model.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Outputs, FType=LayerSelectionType.Single_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        \n",
    "Output_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is ready\n",
      "Creating Network With Dropout started...\n",
      "Training Network started at 2022-05-18 22:19:08.080407....\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 68s 138ms/step - loss: 0.2412 - accuracy: 0.9250 - val_loss: 0.1296 - val_accuracy: 0.9604\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 69s 147ms/step - loss: 0.1018 - accuracy: 0.9696 - val_loss: 0.0821 - val_accuracy: 0.9762\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 69s 147ms/step - loss: 0.0784 - accuracy: 0.9754 - val_loss: 0.0693 - val_accuracy: 0.9788\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 71s 151ms/step - loss: 0.0616 - accuracy: 0.9809 - val_loss: 0.0714 - val_accuracy: 0.9790\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 72s 154ms/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0636 - val_accuracy: 0.9816\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 70s 150ms/step - loss: 0.0437 - accuracy: 0.9862 - val_loss: 0.0715 - val_accuracy: 0.9807\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 70s 149ms/step - loss: 0.0388 - accuracy: 0.9881 - val_loss: 0.0754 - val_accuracy: 0.9813\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 70s 149ms/step - loss: 0.0387 - accuracy: 0.9879 - val_loss: 0.0766 - val_accuracy: 0.9803\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 70s 150ms/step - loss: 0.0338 - accuracy: 0.9892 - val_loss: 0.0646 - val_accuracy: 0.9824\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 63s 133ms/step - loss: 0.0303 - accuracy: 0.9906 - val_loss: 0.0734 - val_accuracy: 0.9822\n",
      "evaluating ... \n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0734 - accuracy: 0.9822\n",
      "Saving Model\n"
     ]
    }
   ],
   "source": [
    "model=MLP()\n",
    "model.PrepareDataset()\n",
    "model.CreateNetwork_With_Dropout()\n",
    "model.Train(10)\n",
    "model.Test()\n",
    "model.SaveModel(r'model\\model_withdropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network With Dropout started...\n",
      "dataset is ready\n",
      "results for  10 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.0710 - accuracy: 0.9818\n",
      "results for  11 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.0708 - accuracy: 0.9817\n",
      "results for  12 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 30ms/step - loss: 0.0670 - accuracy: 0.9816\n",
      "results for  13 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.0641 - accuracy: 0.9815\n",
      "results for  14 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0642 - accuracy: 0.9812\n",
      "results for  15 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.0645 - accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "def Weight_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork_With_Dropout()\n",
    "    model.LoadModel(r'model\\model_withdropout.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Weights, FType=LayerSelectionType.Single_Layer, BitFault=BitFaultType.Zero, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        model.Test()\n",
    "\n",
    "    # for i in [10,11,12,13,14,15]:\n",
    "\n",
    "    #     # injecting weight fault in single layer\n",
    "    #     inject(mlp=model, Injection=InjectionType.Weights, FType=LayerSelectionType.Multiple_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "    #     print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "    #     model.Test()\n",
    " \n",
    "        \n",
    "Weight_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network With Dropout started...\n",
      "dataset is ready\n",
      "results for  10 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 30ms/step - loss: 0.0825 - accuracy: 0.9795\n",
      "results for  11 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.0847 - accuracy: 0.9794\n",
      "results for  12 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.0852 - accuracy: 0.9799\n",
      "results for  13 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 30ms/step - loss: 0.0902 - accuracy: 0.9790\n",
      "results for  14 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.0945 - accuracy: 0.9782\n",
      "results for  15 fault in weights and single layer\n",
      "evaluating ... \n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0981 - accuracy: 0.9779\n"
     ]
    }
   ],
   "source": [
    "def Weight_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork_With_Dropout()\n",
    "    model.LoadModel(r'model\\model_withdropout.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    # for i in [10,11,12,13,14,15]:\n",
    "\n",
    "    #     # injecting weight fault in single layer\n",
    "    #     inject(mlp=model, Injection=InjectionType.Weights, FType=LayerSelectionType.Single_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "    #     print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "    #     model.Test()\n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Weights, FType=LayerSelectionType.Multiple_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "        model.Test()\n",
    " \n",
    "        \n",
    "Weight_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Network With Dropout started...\n",
      "dataset is ready\n",
      "Fault Injection : %100099999999999994results for  10 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  11 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  12 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  13 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  14 fault in weights and single layer\n",
      "Fault Injection : %100099999999999994results for  15 fault in weights and single layer\n"
     ]
    }
   ],
   "source": [
    "def Output_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork_With_Dropout()\n",
    "    model.LoadModel(r'model\\model_withdropout.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Outputs, FType=LayerSelectionType.Single_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "\n",
    "        \n",
    "Output_Fault_Injection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Output_Fault_Injection():\n",
    "    # loading the network\n",
    "    model=MLP()\n",
    "    model.CreateNetwork_With_Dropout()\n",
    "    model.LoadModel(r'model\\model_withdropout.h5')\n",
    "\n",
    "    # loading dataset\n",
    "    model.PrepareDataset()   \n",
    "\n",
    "    # for i in [10,11,12,13,14,15]:\n",
    "\n",
    "    #     # injecting weight fault in single layer\n",
    "    #     inject(mlp=model, Injection=InjectionType.Outputs, FType=LayerSelectionType.Single_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "    #     print(\"results for \",i ,\"fault in weights and single layer\")\n",
    "\n",
    "    for i in [10,11,12,13,14,15]:\n",
    "\n",
    "        # injecting weight fault in single layer\n",
    "        inject(mlp=model, Injection=InjectionType.Outputs, FType=LayerSelectionType.Multiple_Layer, BitFault=BitFaultType.Rand, FaultCount=i)\n",
    "        print(\"results for \",i ,\"fault in output and multi layer\") \n",
    "        \n",
    "Output_Fault_Injection()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c7ecd1d221b6d41596a6c992ff4eb48293873ae9bdd5ab951e201c12d30e9b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
